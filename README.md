Confidence Miscalibration Analysis
Overview

This project studies confidence miscalibration and failure modes in machine learning models.
A convolutional neural network (CNN) is trained on clean data and evaluated under distribution shift to analyze cases where models produce confident but incorrect predictions.

The goal is to understand how prediction confidence aligns (or fails to align) with true correctness, particularly in out-of-distribution (OOD) settings.
